---
title: "Assignment 4:  Logistic Regression for Parole Reform"
author: "Zhanchao Yang"
date: "2025-04-04"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen = 999)
library(tidyverse)
library(caret)
library(pROC)
library(knitr)
library(tinytex)

data<-read.csv("data/NIJ_s_Recidivism_Challenge_Full_Dataset_20240407.csv")
```

# Scenario

The Governor of Georgia wants to replace subjective parole decisions with a consistent, data-driven policy. You’ve been hired to develop a logistic regression model to predict the risk of recidivism — and recommend a cutoff value that will be adopted statewide.

# Sensitivity vs. Specificity

- Sensitivity: Sensitivity represents the proportion of actual positive cases that the model correctly identifies as positive. A high sensitivity indicates the model has a low rate of false negatives, meaning it more accurately identifies individuals who belong to the positive case. In this specific scenario, sensitivity measures the percentage of recidivists that are correctly predicted as recidivists.In other words, the model detects well on individuals who are likely to re-offend.
- Specificity: Specificity measures the proportion of actual negative cases that the model correctly predicts as negative. A high specificity indicates the model has a low rate of false positives, meaning identifies individuals who do not belong to the positive case. In this specific scenario, specificity measures the percentage of non-recidivists that are correctly predicted as non-recidivists. In other words, the model detects well on individuals who are unlikely to re-offend.

In my opinion, sensitivity should be prioritized over specificity in this scenario. In parole reform, the primary goal is to reduce recidivism and ensure that individuals released from prison do not pose a threat to public safety. Prioritizing sensitivity means that we take extra caution by keeping those who are likely to re-offend in custody, even if it detains some individuals who might not actually commit another crime. The government could then offer compensation to those later proven innocent after the jury and trial.

In a different scenario, specificity may be more important than the sensitivity. For example, some crucial resources like prison are limited or government is losing trust from the public as too many innocent people was detained. In this case, the government may want to prioritize specificity to ensure that individuals who are not likely to re-offend are not unnecessarily detained. This would help maintain public trust and ensure that resources are allocated efficiently.

# Data exploration and Data cleaning

## Data cleaning

```{r}
recidivism_ga <- data %>%
  mutate(recidivism_yes = if_else(Recidivism_Within_3years == "true", 1, 0))

recidivism_ga <- na.omit(recidivism_ga)
```

## Training and Testing Partition
```{r}
set.seed(1234)
trainIndex <- createDataPartition(as.factor(recidivism_ga$recidivism_yes), p = 0.7, list = FALSE)
train <- recidivism_ga[trainIndex, ]
test <- recidivism_ga[-trainIndex, ]
```

## Key predictors

- **Age at Release**: Age at release is a significant predictor of recidivism. Younger individuals are more likely to re-offend compared to older individuals.

```{r, echo=FALSE, fig.width=7, fig.height=4}
ggplot(recidivism_ga, aes(x = Age_at_Release, fill = factor(recidivism_yes))) +
  geom_bar(stat = "count", position = "dodge", alpha = 0.5) +
  labs(title = "Age at Release vs Recidivism",
       x = "Age at Release",
       y = "Count",
       fill = "Recidivism") +
  theme_minimal()


```

- **Gang Affiliated**: Gang affiliation is a significant predictor of recidivism. Individuals who are affiliated with gangs are more likely to re-offend compared to those who are not.

```{r, echo=FALSE, fig.width=7, fig.height=4}
ggplot(recidivism_ga, aes(x = Gang_Affiliated, fill = factor(recidivism_yes))) +
  geom_bar(stat = "count", position = "dodge", alpha = 0.5) +
  labs(title = "Gang Affiliated vs Recidivism",
       x = "Gang Affiliated",
       y = "Count",
       fill = "Recidivism") +
  theme_minimal()
```

- **Percent Days Employed**: Percent days employed is a significant predictor of recidivism. Individuals who are employed for a higher percentage of days are less likely to re-offend compared to those who are unemployed.

```{r, echo=FALSE, fig.width=7, fig.height=4}
ggplot(recidivism_ga, aes(x = factor(recidivism_yes), y = Percent_Days_Employed, fill = factor(recidivism_yes))) +
  geom_boxplot(width=0.2) +
  labs(title = "Percent Days Employed vs Recidivism",
       x = "Recidivism",
       y = "Percent Days Employed") +
  theme_minimal()+
  theme(legend.position = "none")
```

- **Education Level**: Education level is a significant predictor of recidivism. Individuals with higher education levels are less likely to re-offend compared to those with lower education levels.
```{r, echo=FALSE, fig.width=7, fig.height=4}
ggplot(recidivism_ga, aes(x = Education_Level, fill = factor(recidivism_yes))) +
  geom_bar(stat = "count", position = "dodge", alpha = 0.5) +
  labs(title = "Education Level vs Recidivism",
       x = "Education Level",
       y = "Count",
       fill = "Recidivism") +
  theme_minimal()
```

- **Prison Offense**: Prison offense is a significant predictor of recidivism. Individuals with more serious offenses like murder are more likely to re-offend compared to those with less serious offenses.
```{r, echo=FALSE, fig.width=7, fig.height=4}
ggplot(recidivism_ga, aes(x = Prison_Offense, fill = factor(recidivism_yes))) +
  geom_bar(stat = "count", position = "dodge", alpha = 0.5) +
  labs(title = "Education Level vs Recidivism",
       x = "Education Level",
       y = "Count",
       fill = "Recidivism") +
  theme_minimal()
```

## Final Model

After testing several key predictors, I got the final model as following:

```{r,warning=FALSE}
model <- glm(recidivism_yes ~ Age_at_Release + Gang_Affiliated + Percent_Days_Employed
             + Prison_Offense + Education_Level ,
             data = train, family = "binomial")
```
```{r}
coeff_table <- summary(model)$coefficients

# Calculate the Odds Ratios by exponentiating the coefficient estimates
odds_ratio <- exp(coeff_table[, "Estimate"])

# Append the odds_ratio as a new column to your coefficients table
coeff_table <- cbind(coeff_table, Odds_Ratio = odds_ratio)

kable(coeff_table,
      caption = "Logistic Regression Coefficient Summary",
      digits = 3)
```

### Interpretation of the coefficients

- **Age at Release: Age at release 43-47**: An odd ratio 0.652 indicates that individuals aged 43-47 are approximately 0.65 times of the odds of recidivism compared to individuals aged 18-22 (the reference category). In other words, older individuals is associated with a lower likelihood of recidivism as the odd ratio less than 1.
- **Gang Affiliated: Yes**: An odd ratio 3.735 indicates that individuals who are gang affiliated are approximately 3.735 times of the odds of recidivism compared to individuals who are unknown. In other words, gang affiliation is associated with a higher likelihood of recidivism as the odd ratio greater than 1.
- **Percent Days Employed**: An odd ratio 0.253 indicates that individuals with a 1% increase in the percentage of days employed are approximately 0.253 times of the odds of recidivism compared to individuals with a lower percentage of days employed. In other words, higher employment is associated with a lower likelihood of recidivism as the odd ratio less than 1.
- **Education Level: less than high school diploma**: An odd ratio 1.220 indicates that individuals with less than a high school diploma are approximately 1.220 times of the odds of recidivism compared to individuals with a bachelor degree (the reference category). In other words, lower education level is associated with a higher likelihood of recidivism as the odd ratio greater than 1.

# Cuteoff Exploration

## Threshold 0.25 and model evaluation
```{r, meassage=FALSE}
test_probs1 <- predict(model, newdata = test, type = "response")
threshold1 <- 0.25
test_preds1 <- ifelse(test_probs1 > threshold1, 1, 0)
confusionMatrix(as.factor(test_preds1), as.factor(test$recidivism_yes), positive = "1")
```

## Threshold 0.5 and model evaluation
```{r}
test_probs2 <- predict(model, newdata = test, type = "response")
threshold2 <- 0.5
test_preds2 <- ifelse(test_probs2 > threshold2, 1, 0)
confusionMatrix(as.factor(test_preds2), as.factor(test$recidivism_yes), positive = "1")
```

## Threshold 0.75 and model evaluation
```{r}
test_probs3 <- predict(model, newdata = test, type = "response")
threshold3 <- 0.75
test_preds3 <- ifelse(test_probs3 > threshold3, 1, 0)
confusionMatrix(as.factor(test_preds3), as.factor(test$recidivism_yes), positive = "1")
```

## Threshold Comparison and AUC
```{r, echo=FALSE}
# Load required library
library(caret)
library(knitr)

# Assume your model has been trained and you already have test_probs3 computed as:
test_probs4 <- predict(model, newdata = test, type = "response")

# Define the thresholds we want to compare
thresholds <- c(0.25, 0.5, 0.75)

# Initialize an empty data frame to store the performance metrics
results <- data.frame(Threshold = numeric(),
                      Accuracy = numeric(),
                      Sensitivity = numeric(),
                      Specificity = numeric(),
                      stringsAsFactors = FALSE)

# Loop over each threshold
for(th in thresholds) {
  # Create predicted classes based on the current threshold
  test_preds <- ifelse(test_probs4 > th, 1, 0)
  
  # Get the confusion matrix for the current threshold
  cm <- confusionMatrix(as.factor(test_preds), as.factor(test$recidivism_yes), positive = "1")
  
  # Append the metrics to the results data frame
  results <- rbind(results,
                   data.frame(Threshold = th,
                              Accuracy = round(cm$overall["Accuracy"], 3),
                              Sensitivity = round(cm$byClass["Sensitivity"], 3),
                              Specificity = round(cm$byClass["Specificity"], 3)))
}
row.names(results) <- c("Threshold 1", "Threshold 2", "Threshold 3")
# Display the results in a nice table
kable(results, caption = "Comparison of Model Performance at Different Thresholds")

```

```{r, message=FALSE}
roc_obj <- roc(test$recidivism_yes, test_probs1)
plot(roc_obj, col = "blue", main = "ROC Curve with AUC", print.auc = TRUE, print.auc.x = 0.5, print.auc.y = 0.3)
```

