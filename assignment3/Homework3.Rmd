---
title: "HW 3: Linear Regression Homework"
author: "Zhanchao Yang"
date: "`r Sys.Date()`"
output: 
  html_document:
    theme: journal
    highlight: tango
    toc: true
    toc_float: 
      collapsed: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# **Introduction**
This assignment explores linear regression modeling using a new dataset, focusing on assumption checks and model interpretation. Students will fit a regression model, check assumptions, and interpret the results in a structured, easy-to-grade format.

---

```{r load_packages, include=FALSE, message = FALSE}
#install.packages(c("tidyverse", "broom", "ggplot2", "caret", "car", "MASS"))
library(tidyverse)
library(broom)
library(ggplot2)
library(caret)
library(car)
library(MASS)
```


```{r load_data, include= FALSE}
data("Boston")
boston_data <- as_tibble(Boston)
head(boston_data)
```


# **1. Exploratory Data Analysis**
## **1.1 Summary Statistics**
```{r summary_stats, results = "hide"}
summary(boston_data)
```

## **1.2 Scatter Plots for Relationship Exploration**
```{r scatter_plots, warning = FALSE, message = FALSE}
ggplot(boston_data, aes(x = rm, y = medv)) +
  geom_point() +
  geom_smooth(method = "lm", color = "red") +
  labs(title = "Relationship Between Number of Rooms and Median Home Value")
```

---

# **2. Fit a Linear Regression Model**
The response variable is `medv` (median home value), and predictors include `rm` (average rooms per house), `lstat` (percentage of lower-income population), and `crim` (crime rate per capita).
```{r linear_regression}
model <- lm(medv ~ rm + lstat + crim, data = boston_data)
summary(model)
```


---

# **3. Checking OLS Assumptions**
## **3.1 Residual Diagnostics**
```{r residual_diagnostics, fig.width=9, fig.height=7}
par(mfrow = c(2, 2))
plot(model)
```
- **Residuals vs Fitted Plot**: If a pattern exists, non-linearity may be present.
- **Q-Q Plot**: Checks if residuals are normally distributed.
- **Scale-Location Plot**: Detects heteroscedasticity.
- **Residuals vs Leverage**: Identifies influential observations.

## **3.2 Multicollinearity Check**
```{r vif_check}
vif(model)
```
- VIF values above **5** indicate high multicollinearity.

---

# **4. Model Refinement: Log Transformations**
If assumptions are violated, log-transforming variables can improve the model.
```{r log_transform}
boston_data <- boston_data %>% mutate(
  log_medv = log(medv),
  log_lstat = log(lstat + 1)
)

log_model <- lm(log_medv ~ rm + log_lstat + crim, data = boston_data)
summary(log_model)
```
- Compare the R² of both models. Does the transformation improve fit?

---

# **5. Model Performance & Cross-Validation**
```{r cross_validation}
set.seed(123)
train_control <- trainControl(method = "cv", number = 10)
cv_model <- train(medv ~ rm + lstat + crim, data = boston_data, method = "lm", trControl = train_control)
cv_model
```

---

# *6. Homework Assignment (10 Points)**
Answer the following questions and submit your answer in a nice R Markdown file.

## **Part 1: Model Interpretation (4 Points)**
1. What does the coefficient for `rm` mean in the original model? (1 point)

- The coefficient for `rm` means that for each additional room in a house, the median home value increases by coefficient units, holding other variables constant. In this case, the coefficient is 5.2169, suggesting that each additional room increases the median home value by $5,216.90 on average, with holding other predictors constant.

2. How does `lstat` impact median home value, and why? (1 point)

- The coefficient of the `lstat` is less than 0, which means that the percentage of the lower-income residents is negatively correlate with the median home values. The residents with lower income tends to live in the neighborhood with lower home values. 
- This negative relationship is expected, because the lower-income residents may not be able to afford expensive houses, so they tend to live in the neighborhood with lower home values.

3. Is crime rate (`crim`) statistically significant? Justify using the p-value. (1 point)

- Yes, the crime rate `crim` is statistically significant. 
- The p-value for the crime rate coefficient is 0.00139, which is below the 0.05 threshold. This indicates that the crime rate has a statistically significant impact on median home values.

4. How well does the original model explain home values (interpret R² and Adjusted R²)? (1 point)

- The original model explains 64.6% of the variance in median home values, as indicated by the R² value. The adjusted R² value accounts for the number of predictors in the model. In this case, the adjusted R² value indicates that the model explains 64.4% of the variance in median home values. Overall, the model has a good fit, but still one-third of the variation in home values has not explained by the model.


## **Part 2: Assumption Checks & Model Improvement (4 Points)**

5. Based on the residual diagnostics, are there any violations of OLS assumptions? (1 point)

- The residual diagnostics plots show that the residuals vs. fitted plot has a slight pattern, indicating that the relationship between the predictors and the response variable may not be linear. The ideal plot should have no pattern and a straight red-line along 0, indicate a linear relationship between predictors and response variable.
- The Q-Q plot shows that the residuals are not perfectly normally distributed, as the points deviate from the line. In general sense, the residuals is generally normally distributed.
- The scale-location plot shows that the residuals are general homoscedastic, with few outliers around 20. Residuals are generally spread out evenly across the range of fitted values. 

In summary,the models met the homoscedasticity and normality of the residuals assumptions, but there is a slight violation of the linearity assumption. The multicollinearity check will be conducted via the VIF test.

6. What does the VIF test indicate about multicollinearity? (1 point)
- Based on the VIF test, there are no obvious multicollinearity issues in the model. All VIF values are below 5, indicating that the predictors are not highly correlated with each other.
- This suggests that the model met the OLS regression assumption of the no multicollinearity among predictors.

7. After log-transforming `lstat`, does model performance improve? Explain. (1 point)

Yes, after log-transforming `lstat`, the model performance  improve. The adjusted R² value of the log-transformed model is 0.7415, which is higher than the original model's adjusted R² value of 0.6437. This indicate the new log-transformed model explains 74.15% of the variance in logged median household values, while the original model could only explain 64.37% of the variance in median home values. The log-transformed model has a better fit than the original model.

8. Compare RMSE from cross-validation to the model's residual standard error. Which suggests better predictive performance? (1 point)

The RMSE from cross-validation is 5.48, while the model's residual standard error is 5.49. The cross validation has a slightly lower RMSE than the model's residual standard error, which suggests that the cross-validation model has a better predictive performance than the original model. The cross-validation was trained on a portion of the data and tested on the remaining data, which avoid the overfitting issue and provide a more accurate estimate of the model's predictive performance.

## **Part 3: Expanding the Model (2 Points)**
9. Add `dis` (distance to employment centers) to the model. Does it improve fit? (1 point)
```{r}
model_dis <- lm(medv ~ rm + lstat + crim + dis, data = boston_data)
summary(model_dis)
```
```{r}
vif(model_dis)
```

Yes, adding the `dis` variable to the model improve the fit. The adjusted R² value of the new model is 0.6549, which is higher than the original model's adjusted R² value of 0.6437. The new model explain more variance in median home values than the original model, which improve the model fits.

10. Try another predictor from the dataset that you think might be relevant. Justify why you selected it and interpret its impact on the model. (1 point)

```{r}
model_new <- lm(medv ~ rm + lstat + crim + +dis+ age, data = boston_data)
summary(model_new)
```

```{r}
vif(model_new)
```
- I added the `age` variable to the model, because I assume that the housing age has potential relationship with the median housing value. 

- The coefficient of the `age` variable is -0.1237, which means that for each additional year of age, the average median home value decreases by 0.027 unit, holding other variables constant. The p-value of the `age` variable is 0.0538, which is slightly above the 0.05 threshold. This indicates that the `age` variable is not statistically significant in predicting median home values.
- However, the adjusted R² value of the new model is 0.6568, which is higher than the original model's adjusted R² value of 0.6549. The new model explain more variance in median home values than the original model.

Overall, as the `age` variable is not statistically significant, it may not be a good predictor for the median home values. Adding `age` variable, only improve the adjusted R² value by 0.0019. I may consider removing the `age` variable from the model.



---
